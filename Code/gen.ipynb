{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of completions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "from soft_embedding import SoftEmbedding\n",
    "\n",
    "random.seed(5678)\n",
    "torch.manual_seed(5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "ops = pickle.load( open( 'data/ops_raw.pickle', \"rb\" ) )\n",
    "\n",
    "with open( 'config/gCONFIG.json', \"r\") as f:\n",
    "    CONFIG = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egs = {}\n",
    "\n",
    "for op in ops.keys():\n",
    "\n",
    "    if op not in ['suggest-rephrase']:\n",
    "        \n",
    "        egs[str(op)] = []\n",
    "        \n",
    "        print(op)\n",
    "        \n",
    "        # Get random subset of 100 test samples.\n",
    "        subset = random.sample(ops[op]['test'], 100)\n",
    "        \n",
    "        make_prompts = ops[op]['make_prompts']\n",
    "        \n",
    "        for s in subset:\n",
    "            ps = make_prompts(s)\n",
    "            ps['ex'] = s\n",
    "            ps['id'] = s['id']\n",
    "            egs[str(op)].append(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-2.7B')\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(model, prompt, CF):\n",
    "\n",
    "    if 'soft' in CF['checkpoint']:\n",
    "        \n",
    "        inputs = tokenizer(prompt[\"in\"], return_tensors=\"pt\")\n",
    "\n",
    "        # need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
    "        # even though it does not matter what you pad input_ids with, it's just to make HF happy\n",
    "        inputs['input_ids'] = torch.cat([torch.full((1,30), 50256), inputs['input_ids']], 1)\n",
    "        inputs['attention_mask'] = torch.cat([torch.full((1,30), 1), inputs['attention_mask']], 1)\n",
    "        \n",
    "        inputs.to(device)\n",
    "        \n",
    "        gen_tokens = model.generate(**inputs, do_sample=True, temperature=0.9, max_new_tokens=150, use_cache=False)\n",
    "        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "        \n",
    "        return gen_text\n",
    "    \n",
    "    else:\n",
    "        input_ids = tokenizer(prompt['in'], return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        # Need to amend input_ids to account for possible soft prompt!\n",
    "\n",
    "        gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_new_tokens=150,)\n",
    "        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "        return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = {}\n",
    "\n",
    "for idx, CF in enumerate(CONFIG):\n",
    "    if bool(CF['useThis']):\n",
    "    \n",
    "        gens[CF['model']] = []\n",
    "\n",
    "        print(f'Generating for model #{idx}')\n",
    "        print(f\"{CF['op']} / {CF['checkpoint']} / {CF['prompt_type']}\")\n",
    "\n",
    "        checkpoint = CF['checkpoint']\n",
    "\n",
    "        print(\". . . loading\")\n",
    "        if 'soft' in checkpoint:\n",
    "            model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "            s_wte = SoftEmbedding(model.get_input_embeddings(), n_tokens=30, initialize_from_vocab=True)\n",
    "            model.set_input_embeddings(s_wte)\n",
    "            model.load_state_dict(torch.load(f\"{CF['checkpoint']}/pytorch_model.bin\"), strict=False)\n",
    "        else: \n",
    "            model = GPTNeoForCausalLM.from_pretrained(checkpoint, use_auth_token=True)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\". . . model load DONE\")\n",
    "\n",
    "        print(\". . . generating\")\n",
    "        for eg in tqdm(egs[CF['op']]):\n",
    "            p = copy.deepcopy(eg)[CF['prompt_type']]\n",
    "            gen_text = generate_completion(model, p, CF)\n",
    "            p['op'] = CF['op']\n",
    "            p['gen'] = gen_text\n",
    "            p['model'] = CF['model']\n",
    "            p['ex'] = copy.deepcopy(eg['ex'])\n",
    "            p['map_title'] = copy.deepcopy(eg['map_title'])\n",
    "            p['id'] = copy.deepcopy(eg['id'])\n",
    "            gens[CF['model']].append(p)\n",
    "\n",
    "        print(\". . . generation DONE\")\n",
    "\n",
    "        with open('gens.json', 'w') as f:\n",
    "            json.dump(gens, f)\n",
    "        \n",
    "        model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy(gen):\n",
    "\n",
    "    text = gen['gen']\n",
    "    \n",
    "    # Remove <|endoftext|> markers (there will be 30 of them at the start for soft prompts).\n",
    "    text = text.replace(\"<|endoftext|>\", '')\n",
    "    \n",
    "    # Remove prompt so we just have the newly generated text.\n",
    "    text = text[len(gen['in']):]\n",
    "\n",
    "    # Remove any parentheses.\n",
    "    text = re.subn(r\"\\(.*?\\)\", '', text)[0]\n",
    "\n",
    "    # Remove any symbols.\n",
    "    text = re.subn(r\"(\\*|\\_|`)\", '', text)[0]\n",
    "    \n",
    "    # Remove leading gibberish.\n",
    "    text = re.sub(r\"^[0-9]\\.\\s\", '', text)\n",
    "    text = re.sub(r\"^[^a-zA-Z0-9\\\"']*\", '', text)\n",
    "    \n",
    "    # Remove trailing whitespace.\n",
    "    text = re.sub(r\"\\s*$\", '', text)\n",
    "    \n",
    "    # Truncate to only one sentence.\n",
    "    lines = text.split('\\n')\n",
    "    sentences = lines[0].split('. ')\n",
    "    if gen['op'] != 'suggest-intermediary-claims':\n",
    "        if len(sentences) > 1:\n",
    "            text = sentences[0] + '.'\n",
    "        else:\n",
    "            text = sentences[0]\n",
    "    else:\n",
    "        text = lines[0]\n",
    "        \n",
    "    # Capitalise first letter.\n",
    "    if len(text) > 0:\n",
    "        text = text[0].upper() + text[1:]\n",
    "    \n",
    "    gen['gen'] = text\n",
    "    return gen\n",
    "\n",
    "for model in tqdm(gens.keys()):\n",
    "    gens[model] = list(map(tidy, gens[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gens_tidy.json', 'w') as f:\n",
    "    json.dump(gens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a gens_only copy for ease of interactive inspection.\n",
    "\n",
    "import copy\n",
    "gens_only = copy.deepcopy(gens)\n",
    "for model in tqdm(gens_only.keys()):\n",
    "    gens_only[model] = [gen['gen'] for gen in gens_only[model]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert human responses so we have a benchmark.\n",
    "\n",
    "gens['aH'] = copy.deepcopy(gens['a1'])\n",
    "gens['bH'] = copy.deepcopy(gens['b1'])\n",
    "gens['cH'] = copy.deepcopy(gens['c1'])\n",
    "gens['dH'] = copy.deepcopy(gens['d1'])\n",
    "\n",
    "for model in ['aH', 'bH', 'cH', 'dH']:\n",
    "    for gen in gens[model]:\n",
    "        gen['gen'] = gen['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in tqdm(gens.keys()):\n",
    "    if gens[model][0]['op'] == 'suggest-intermediary-claims':\n",
    "        for gen in gens[model]:\n",
    "            gen['gen'] = gen['gen'].split(' ~ ')\n",
    "        gens[model] = [gen for gen in gens[model] if len(gen['gen']) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse into single list, randomly (but nicely) ordered.\n",
    "\n",
    "gens_flat = []\n",
    "for model in gens.keys():\n",
    "    gens_flat = gens_flat + gens[model]\n",
    "    \n",
    "random.shuffle(gens_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set([gen['id'] for gen in gens_flat]))\n",
    "random.shuffle(ids)\n",
    "gens_flat = sorted(gens_flat, key=lambda gen: (ids.index(gen['id']), gen['model'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gens_tidy_official.json', 'w') as f:\n",
    "    json.dump(gens_flat, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
