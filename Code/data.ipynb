{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from math import factorial\n",
    "import random\n",
    "\n",
    "random.seed(5678)\n",
    "np.random.seed(5678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/home/luke/Archive/200—Work/HuntLab/2021 ~ Argument Processor/Data/\"\n",
    "path_to_models = \"/home/luke/Archive/200—Work/HuntLab/2021 ~ Argument Processor/Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = {\n",
    "    \"suggest-intermediary-claims\": {},\n",
    "    \"suggest-reasons\": {},\n",
    "    \"suggest-objections\": {},\n",
    "    \"suggest-conclusion\": {},\n",
    "    \"suggest-copremise\": {},\n",
    "    \"suggest-abstraction\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for stripping markdown markup from a string.\n",
    "\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import complete Kialo maps.\n",
    "\n",
    "path = path_to_data + \"Kialo—Lenz-2020/Raw-Export/\"\n",
    "\n",
    "fs = [f for f in listdir(path) if f[-4:] == \".txt\" and \"(1)\" not in f]\n",
    "    # Only include text files (not system files).\n",
    "    # Exclude files that are duplicates (those that have \"(1)\" in the filename).\n",
    "\n",
    "def make_claim(claim):\n",
    "\n",
    "    claim = re.search('((\\d+\\.)+) (Pro: |Con: )?(.*)', claim)\n",
    "    \n",
    "    if claim == None:\n",
    "        return {\n",
    "            \"id\": None\n",
    "        }\n",
    "    else:\n",
    "        claim = claim.groups()\n",
    "        return {\n",
    "            \"id\": claim[0],\n",
    "            \"type\": claim[-2][:3] if claim[-2] != None else \"Seed\",\n",
    "            \"txt\": unmark(claim[-1])\n",
    "        }\n",
    "\n",
    "def is_child(d, id):\n",
    "    if len(d) < len(id):\n",
    "        return False\n",
    "    elif len(d.split('.')) != len(id.split('.')) + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return d[:len(id)] == id\n",
    "\n",
    "def make_map(filename):\n",
    "    \n",
    "    with open(path + filename, 'r', encoding='utf-8') as file:\n",
    "        txt = file.read()\n",
    "    \n",
    "    # Extract title.\n",
    "    title = txt.split('\\n', 1)[0][18:]\n",
    "    \n",
    "    # Get array of claims.\n",
    "    claims = '\\n'.join(txt.splitlines()[2:]).split('\\n1.')\n",
    "    seed = claims[0]\n",
    "    claims = [seed] + ['1.' + s for s in claims[1:]]\n",
    "\n",
    "    # Reformat each claim.\n",
    "    claims = [make_claim(claim) for claim in claims]\n",
    "    claims = [claim for claim in claims if claim[\"id\"] != None]\n",
    "    \n",
    "    # Convert to dictionary.\n",
    "    m = {claim[\"id\"]: claim for claim in claims}\n",
    "\n",
    "    # Store ids of children.\n",
    "    ids = [claim[\"id\"] for claim in claims]\n",
    "    for id in ids:\n",
    "        m[id][\"claims\"] = [d for d in ids if is_child(d, id)]\n",
    "\n",
    "    # Resolve references to other claims.\n",
    "    n_references = 0\n",
    "    for id in ids:\n",
    "        if m[id][\"txt\"][:7] == \"-> See \":\n",
    "            n_references = n_references + 1\n",
    "            ref_id = m[id][\"txt\"][7:]\n",
    "            if ref_id in ids:\n",
    "                m[id][\"txt\"] = m[ref_id][\"txt\"]\n",
    "            else:\n",
    "                m[id][\"txt\"] = ref_id.split(' ', 2)[2]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"filename\": filename,\n",
    "        \"ids\": ids,\n",
    "        \"map\": m,\n",
    "        \"n_references\": n_references\n",
    "    }\n",
    "\n",
    "maps = [ make_map(f) for f in tqdm(fs) ]\n",
    "\n",
    "# Save maps to file.\n",
    "\n",
    "with open(path_to_models + 'maps.json', 'w') as fp:\n",
    "    json.dump(maps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in maps.\n",
    "\n",
    "with open(path_to_models + 'maps.json') as json_file:\n",
    "    maps = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate maps to maximum depth of 4.\n",
    "\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all newline characters from claims.\n",
    "\n",
    "for map in tqdm(maps):\n",
    "    for id in map['ids']:\n",
    "        map['map'][id]['txt'] = map['map'][id]['txt'].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign maps to train/val/test splits.\n",
    "\n",
    "# Randomly re-order examples.\n",
    "random.shuffle(maps)\n",
    "revisions = revisions.sample(frac=1, random_state=5678)\n",
    "\n",
    "# Assign to splits.\n",
    "\n",
    "revisions_train, revisions_validate, revisions_test = np.split(revisions, [int(.6 * len(revisions)), int(.8 * len(revisions))])\n",
    "revisions = {\n",
    "    \"train\": revisions_train,\n",
    "    \"validate\": revisions_validate,\n",
    "    \"test\": revisions_test\n",
    "}\n",
    "\n",
    "maps_train, maps_validate, maps_test = np.split(maps, [int(.6 * len(maps)), int(.8 * len(maps))])\n",
    "maps = {\n",
    "    \"train\": maps_train,\n",
    "    \"validate\": maps_validate,\n",
    "    \"test\": maps_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract forks and branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract forks.\n",
    "\n",
    "forks = {\n",
    "    \"train\": [],\n",
    "    \"validate\": [],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "def get_forks(map):\n",
    "    \n",
    "    forks = []\n",
    "\n",
    "    for id in map[\"ids\"]:\n",
    "        if len(map[\"map\"][id][\"claims\"]) > 0 and len(id.split('.')) <= max_depth:\n",
    "            forks.append({\n",
    "                \"seed\": map[\"map\"][id],\n",
    "                \"claims\": [map[\"map\"][d] for d in map[\"map\"][id][\"claims\"]],\n",
    "                \"map_title\": map['map']['1.']['txt']\n",
    "            })\n",
    "\n",
    "    return forks\n",
    "\n",
    "for split in maps.keys():\n",
    "    for m in tqdm(maps[split]):\n",
    "        forks[split].extend(get_forks(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract branches.\n",
    "\n",
    "branches = {\n",
    "    \"train\": [],\n",
    "    \"validate\": [],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "def get_branches(branch, map, split):\n",
    "    \"Recursive function to get all chains of reasoning.\"\n",
    "    \n",
    "    if len(branch[-1][\"claims\"]) == 0:\n",
    "        branches[split].append({ \"branch\": branch, \"map_title\": map['map']['1.']['txt'] })\n",
    "    \n",
    "    elif len([ id for id in branch[-1][\"claims\"] if map[\"map\"][id][\"type\"] == \"Pro\"]) == 0:\n",
    "        branches[split].append({ \"branch\": branch, \"map_title\": map['map']['1.']['txt'] })\n",
    "    \n",
    "    else:\n",
    "        leaf = branch[-1]\n",
    "        for id in leaf[\"claims\"]:\n",
    "            if map[\"map\"][id][\"type\"] == \"Pro\":\n",
    "                get_branches(branch + [map[\"map\"][id]], map, split)\n",
    "\n",
    "for split in maps.keys():\n",
    "    for m in maps[split]:\n",
    "        seed_branch = [m[\"map\"][\"1.\"]]\n",
    "        get_branches(seed_branch, m, split)\n",
    "\n",
    "    # Augment with all sub-chains of length 3 or more.\n",
    "   \n",
    "    sub_branches = []\n",
    "    for branch in branches[split]:\n",
    "        if len(branch['branch']) < 4:\n",
    "            next\n",
    "        else:\n",
    "            for start in range(0, len(branch['branch'])-2):\n",
    "                for end in range(start+3, len(branch['branch'])+1):\n",
    "                    sub_branches.append({ \"branch\": branch['branch'][start:end], \"map_title\": branch['map_title']})\n",
    "\n",
    "    branches[split] = sub_branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate task-specific datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-intermediary-claims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-intermediary-claims\"\n",
    "\n",
    "# Create dataset.\n",
    "\n",
    "def make_datum(b):\n",
    "    d = {\n",
    "        \"in\": {\n",
    "            \"start\": b['branch'][-1],\n",
    "            \"end\": b['branch'][0]\n",
    "        },\n",
    "        \"out\": b['branch'][::-1],\n",
    "        \"map_title\": b['map_title']\n",
    "    }\n",
    "    return(d)\n",
    "\n",
    "for split in branches.keys():\n",
    "    ops[op][split] = [make_datum(b) for b in branches[split]]\n",
    "\n",
    "# Define prompt-generation function(s).\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts['map_title'] = d[\"map_title\"]\n",
    "    \n",
    "    prompts[\"manual\"] = {\n",
    "        \"in\": f'Input: {d[\"in\"][\"start\"][\"txt\"]} -> {d[\"in\"][\"end\"][\"txt\"]}\\n\\nOutput: ',\n",
    "        \"out\": \" -> \".join([claim[\"txt\"] for claim in d[\"out\"]])\n",
    "    }\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": f\"Reason from the start claim to the end claim.\\n\\nStart and end claim: A cyclone hit Queensland, Australia. ~ The price of bananas increased.\\nCompleted chain of reasoning:\\n* A cyclone hit Queensland, Australia. ~ The cyclone destroyed banana crops. ~ Supply of bananas went down, whilst demand stayed constant. ~ The price of bananas increased.\\n\\nStart and end claim: Education levels improve. ~ Society becomes more politically polarised.\\nCompleted chain of reasoning:\\n* Education levels improve. ~ People become more skilled at finding high-quality justifications for their existing beliefs (confirmation bias). ~ Society becomes more politically polarised.\\n\\nStart and end claim: People move out of cities and into the countryside. ~ Greenhouse gas emissions increase.\\nCompleted chain of reasoning:\\n* People move out of cities and into the countryside. ~ Population density decreases. ~ Both people and products need to be transported further. ~ They are transported using vehicles that burn fossil fuels. ~ Greenhouse gas emissions increase.\\n\\nStart and end claim: {d['in']['start']['txt']} ~ {d['in']['end']['txt']}\\nCompleted chain of reasoning\\n*\",\n",
    "        \"out\": \" ~ \".join([claim[\"txt\"] for claim in d[\"out\"]])\n",
    "    }\n",
    "\n",
    "    prompts[\"soft\"] = {\n",
    "        \"in\": f'{d[\"in\"][\"start\"][\"txt\"]} -> {d[\"in\"][\"end\"][\"txt\"]}\\n\\nAnswer: ',\n",
    "        \"out\": \" -> \".join([claim[\"txt\"] for claim in d[\"out\"]])\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-reasons`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-reasons\"\n",
    "\n",
    "# Create dataset.\n",
    "\n",
    "def sub_lists(l):\n",
    "    lists = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            lists.append(l[j: i])\n",
    "    return lists\n",
    "\n",
    "def make_data(v):\n",
    "    ds = []\n",
    "\n",
    "    pros = [claim for claim in v[\"claims\"] if claim[\"type\"] == \"Pro\"]\n",
    "    pros_sublists = sub_lists(pros)\n",
    "\n",
    "    for sublist in pros_sublists:\n",
    "        for p_idx in range(min(10, factorial(len(sublist)))):\n",
    "            # Randomly generate at most 10 permutations of the subset.\n",
    "            # The number of permutations is capped to prevent factorial explosion in runtime.\n",
    "            \n",
    "            permutation = list(np.random.permutation(sublist))\n",
    "\n",
    "            ds.append({\n",
    "                \"in\": {\n",
    "                    \"seed\": v[\"seed\"],\n",
    "                    \"pros\": permutation[:-1]\n",
    "                },\n",
    "                \"out\": permutation[-1],\n",
    "                \"map_title\": v[\"map_title\"]\n",
    "            })\n",
    "    \n",
    "    return(ds)\n",
    "\n",
    "for split in forks.keys():\n",
    "    forks_with_pros = [v for v in forks[split] if len([claim for claim in v[\"claims\"] if claim[\"type\"] == \"Pro\"]) > 0]\n",
    "    data = [make_data(v) for v in tqdm(forks_with_pros)]\n",
    "    data = list(chain.from_iterable(data))\n",
    "    ops[op][split] = data\n",
    "\n",
    "# Define prompt-generation function(s).\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts[\"map_title\"] = d[\"map_title\"]\n",
    "\n",
    "    prompts[\"manual\"] = {\n",
    "        \"in\": d[\"in\"][\"seed\"][\"txt\"] + \"\\n\\nPros:\\n- \" + \"\\n- \".join([claim['txt'] for claim in d[\"in\"][\"pros\"]]) + \"\\n- \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"zero-shot\"] = {\n",
    "        \"in\": \"List reasons why: \\\"\" + d['in']['seed']['txt'] + \"\\n\\nReasons:\" + ''.join(['\\n* ' + claim['txt'] for claim in d['in']['pros']]) +\"\\n* \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": \"Suggest reasons why each claim is true.\\n\\nClaim: The COVID-19 pandemic was caused by a lab leak in Wuhan.\\nReason 1: A virology lab in Wuhan was conducted research on coronaviruses.\\nReason 2: Biosecurity leaks are relatively common.\\n\\nClaim: The world should transition away from fossil fuels.\\nReason 1: The burning of fossil fuels releases greenhouse gases.\\nReason 2: Greenhouse gases warm the planet and lead to sea level rise.\\nReason 3: Burning fossil fuels leads to air pollution and preventable deaths.\\n\\nClaim: Socrates is mortal.\\nReason 1: Socrates is human.\\nReason 2: All humans are mortal.\\n\\nClaim: \" + d['in']['seed']['txt'] + ''.join([\"\\nReason \" + str(index + 1) + \": \" + claim['txt'] for index, claim in enumerate(d['in']['pros'])]) + \"\\nReason \" + str(len(d['in']['pros']) + 1) + \": \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"soft\"] = {\n",
    "        \"in\": d[\"in\"][\"seed\"][\"txt\"] + \"\\n\\nPros:\\n- \" + \"\\n- \".join([claim['txt'] for claim in d[\"in\"][\"pros\"]]) + \"\\n- \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-objections`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-objections\"\n",
    "\n",
    "# Create dataset.\n",
    "\n",
    "def make_data(v):\n",
    "    ds = []\n",
    "\n",
    "    cons = [claim for claim in v[\"claims\"] if claim[\"type\"] == \"Con\"]\n",
    "    cons_sublists = sub_lists(cons)\n",
    "\n",
    "    for sublist in cons_sublists:\n",
    "        for p_idx in range(min(10, factorial(len(sublist)))):\n",
    "            # Randomly generate at most 10 permutations of the subset.\n",
    "            # The number of permutations is capped to prevent factorial explosion in runtime.\n",
    "            \n",
    "            permutation = list(np.random.permutation(sublist))\n",
    "\n",
    "            ds.append({\n",
    "                \"in\": {\n",
    "                    \"seed\": v[\"seed\"],\n",
    "                    \"cons\": permutation[:-1]\n",
    "                },\n",
    "                \"out\": permutation[-1],\n",
    "                \"map_title\": v[\"map_title\"]\n",
    "            })\n",
    "    \n",
    "    return(ds)\n",
    "\n",
    "for split in forks.keys():\n",
    "    forks_with_cons = [v for v in forks[split] if len([claim for claim in v[\"claims\"] if claim[\"type\"] == \"Con\"]) > 0]\n",
    "    data = [make_data(v) for v in tqdm(forks_with_cons)]\n",
    "    data = list(chain.from_iterable(data))\n",
    "    ops[op][split] = data\n",
    "\n",
    "# Define prompt-generation function(s).\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts[\"map_title\"] = d[\"map_title\"]\n",
    "\n",
    "    prompts[\"manual\"] = {\n",
    "        \"in\": d[\"in\"][\"seed\"][\"txt\"] + \"\\n\\nCons:\\n- \" + \"\\n- \".join([claim['txt'] for claim in d[\"in\"][\"cons\"]]) + \"\\n- \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"zero-shot\"] = {\n",
    "        \"in\": \"List objections to the claim that: \\\"\" + d['in']['seed']['txt'] + \"\\n\\nObjections:\" + ''.join(['\\n* ' + claim['txt'] for claim in d['in']['cons']]) +\"\\n* \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": \"Suggest reasons why each claim is false.\\n\\nClaim: The COVID-19 pandemic was caused by a lab leak in Wuhan.\\nReason 1: Viral incursions from animals to humans are common.\\nReason 2: There was a wild animal market in Wuhan that sold bats for human consumption.\\n\\nClaim: The world should transition away from fossil fuels.\\nReason 1: Fossil fuels provide cheap energy that helps lift people out of poverty.\\nReason 2: There is ongoing debate about whether fossil fuels are responsible for climate change.\\nReason 3: The fossil fuel industry is a major employer in disadvantaged rural areas.\\n\\nClaim: Socrates is mortal.\\nReason 1: Socrates may be a fictional character.\\nReason 2: Socrates is still spoken about milennia after he allegedly died.\\n\\nClaim: \" + d['in']['seed']['txt'] + ''.join([\"\\nReason \" + str(index + 1) + \": \" + claim['txt'] for index, claim in enumerate(d['in']['cons'])]) + \"\\nReason \" + str(len(d['in']['cons']) + 1) + \": \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"soft\"] = {\n",
    "        \"in\": d[\"in\"][\"seed\"][\"txt\"] + \"\\n\\nCons:\\n- \" + \"\\n- \".join([claim['txt'] for claim in d[\"in\"][\"cons\"]]) + \"\\n- \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-copremise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-copremise\"\n",
    "\n",
    "for split in maps.keys():\n",
    "    ops[op][split] = []\n",
    "\n",
    "def make_datum(v):\n",
    "    d = {\n",
    "        \"in\": {\n",
    "            \"seed\": v['seed'],\n",
    "            \"reasons\": [claim for claim in v[\"claims\"] if claim[\"type\"] == \"Pro\"]\n",
    "        },\n",
    "        \"map_title\": v[\"map_title\"]\n",
    "    }\n",
    "    return(d)\n",
    "\n",
    "forks_with_pros = [v for v in forks['test'] if len([claim for claim in v[\"claims\"] if claim[\"type\"] == \"Pro\"]) > 0]\n",
    "ops[op]['test'] = [make_datum(v) for v in forks_with_pros]\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts[\"map_title\"] = d[\"map_title\"]\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": \"Identify assumptions with the following arguments.\\n\\nPremises:\\n* All men are mortal.\\nConclusion: Socrates is mortal.\\nAssumptions:\\n* Socrates is a man.\\n\\nPremises:\\n* Digital literacy of the population improves.\\nConclusion: Misinformation and conspiracy theories become less prevalent.\\nAssumptions:\\n* The only reason people believe misinformation is that they lack digital literacy.\\n* Improved digital literacy will not exacerbate confirmation bias.\\n\\nPremises:\\n* I want a PhD from a recognised university.\\n* London has lots of organisations and networking opportunities in my field.\\nConclusion: I should do a PhD in London.\\nAssumptions:\\n* I can afford to live in London.\\n* I will have the flexibility to pursue research that interests me.\\n\\nPremises:\\n* \" + '\\n* '.join([claim['txt'] for claim in d['in']['reasons']]) + \"\\nConclusion: \" + d['in']['seed']['txt'] + \"\\nAssumptions:\\n*\"\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-abstraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-abstraction\"\n",
    "\n",
    "for split in maps.keys():\n",
    "    ops[op][split] = []\n",
    "\n",
    "def make_datum(b):\n",
    "    d = {\n",
    "        \"in\": {\n",
    "            \"context\": b['branch'][0],\n",
    "            \"reason\": b['branch'][1]\n",
    "        },\n",
    "        \"map_title\": b[\"map_title\"]\n",
    "    }\n",
    "    return(d)\n",
    "\n",
    "ops[op]['test'] = [make_datum(b) for b in branches['test']]\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts[\"map_title\"] = d[\"map_title\"]\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": \"In the following examples, notice how the first claim is rephrased to be more abstract.\\n\\nToo specific: Nuclear power has very low greenhouse gas emissions. => We should be building more nuclear power plants.\\nBetter: Nuclear power is good for the environment. -> We should be building more nuclear power plants.\\n\\nToo specific: School uniforms ensure that everyone is wearing the same clothes. => Schools should make students wear a uniform.\\nBetter: Uniforms reduce class-based discrimination. => Schools should make students wear a uniform.\\n\\nToo specific: The Thames barrier has 5 backup generators. => The Thames barrier will not fail.\\nBetter: The Thames barrier has been designed with lots of redundancy. => The Thames barrier will not fail.\\n\\nToo specific: \" + d['in']['reason']['txt'] + \" => \" + d['in']['context']['txt'] + \"\\nBetter: \"\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest-conclusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_depth for extracted forks to 6 so that there is more training data for this operation.\n",
    "\n",
    "max_depth = 5\n",
    "\n",
    "forks = {\n",
    "    \"train\": [],\n",
    "    \"validate\": [],\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "def get_forks(map):\n",
    "    \n",
    "    forks = []\n",
    "\n",
    "    for id in map[\"ids\"]:\n",
    "        if len(map[\"map\"][id][\"claims\"]) > 0 and len(id.split('.')) <= max_depth:\n",
    "            forks.append({\n",
    "                \"seed\": map[\"map\"][id],\n",
    "                \"claims\": [map[\"map\"][d] for d in map[\"map\"][id][\"claims\"]],\n",
    "                \"map_title\": map['map']['1.']['txt']\n",
    "            })\n",
    "\n",
    "    return forks\n",
    "\n",
    "for split in maps.keys():\n",
    "    for m in tqdm(maps[split]):\n",
    "        forks[split].extend(get_forks(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = \"suggest-conclusion\"\n",
    "\n",
    "# Create dataset.\n",
    "\n",
    "def make_datum(v):\n",
    "    d = {\n",
    "        \"in\": [claim for claim in v[\"claims\"] if claim[\"type\"] == \"Pro\"],\n",
    "        \"out\": v[\"seed\"],\n",
    "        \"map_title\": v[\"map_title\"]\n",
    "    }\n",
    "    return(d)\n",
    "\n",
    "for split in forks.keys():\n",
    "    data = [make_datum(v) for v in forks[split]]\n",
    "    data = [d for d in data if len(d[\"in\"]) > 0]\n",
    "    ops[op][split] = data\n",
    "\n",
    "# Define prompt-generation function(s).\n",
    "\n",
    "def make_prompts(d):\n",
    "    \n",
    "    prompts = {}\n",
    "\n",
    "    prompts[\"map_title\"] = d[\"map_title\"]\n",
    "\n",
    "    prompts[\"manual\"] = {\n",
    "        \"in\": \"\\n\".join([f\"- {claim['txt']}\" for claim in d[\"in\"]]) + \"\\n\\nConclusion: \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"zero-shot\"] = {\n",
    "        \"in\": \"Consider the facts:\" + ''.join(['\\n* ' + claim['txt'] for claim in d['in']]) + \"\\n\\nWe must conclude that: \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"few-shot\"] = {\n",
    "        \"in\": \"Suggest what we can conclude from each of the following sets of facts.\\n\\nFacts:\\n* Viral incursions from animals to humans are common.\\n* There was a wild animal market in Wuhan that sold bats for human consumption.\\nConclusion: The SARS-CoV-2 coronavirus may have originated in bats.\\n\\nFacts:\\n* Burning fossil fuels releases greenhouse gases that cause climate change.\\n* Climate change may make parts of the planet uninhabitable for humans.\\nConclusion: The world should transition away from fossil fuels.\\n\\nFacts:\\n* Socrates is human.\\n* All humans are mortal.\\nConclusion: Socrates is mortal.\\n\\nFacts:\" + ''.join(['\\n* ' + claim['txt'] for claim in d['in']]) + \"\\nConclusion: \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    prompts[\"soft\"] = {\n",
    "        \"in\": \"\\n\".join([f\"- {claim['txt']}\" for claim in d[\"in\"]]) + \"\\n\\nConclusion: \",\n",
    "        \"out\": d[\"out\"][\"txt\"]\n",
    "    }\n",
    "\n",
    "    return(prompts)\n",
    "\n",
    "ops[op][\"make_prompts\"] = make_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, pre-compile all examples into prompt strings.\n",
    "\n",
    "if False:\n",
    "    for op in ops.keys():\n",
    "        for split in maps.keys():\n",
    "            if 'make_prompts' in ops[op].keys():\n",
    "                make_prompts = ops[op]['make_prompts']\n",
    "                ops[op][split] = [make_prompts(prompt) for prompt in ops[op][split]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate train/test/eval examples to a feasible maximum.\n",
    "\n",
    "for op in ops.keys():\n",
    "    for split in ops[op].keys():\n",
    "        if split == 'train':\n",
    "            if len(ops[op][split]) > 50_000:\n",
    "                ops[op][split] = random.sample(ops[op][split], 50_000)\n",
    "        elif split in ['validate', 'test']:\n",
    "            if len(ops[op][split]) > 10_000:\n",
    "                ops[op][split] = random.sample(ops[op][split], 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique (within each op) IDs for each example.\n",
    "\n",
    "for op in ops.keys():\n",
    "    for split in maps.keys():\n",
    "        for k, prompt in enumerate(ops[op][split]):\n",
    "            prompt['id'] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ops.keys():\n",
    "    print(key)\n",
    "    print(f\"{len(ops[key]['train'])} / {len(ops[key]['validate'])} / {len(ops[key]['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "pickle.dump(ops,\n",
    "            open(path_to_models + 'ops.pickle', 'wb'),\n",
    "            protocol=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
